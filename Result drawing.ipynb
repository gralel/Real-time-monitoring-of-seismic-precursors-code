{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Training and Validation Metrics Analysis\n",
    "\n",
    "This module provides comprehensive performance visualization for deep learning models (GRU, LSTM, RNN, MLP, Transformer) across different temporal windows (7-day, 14-day, 30-day) for seismic-geomagnetic signal recognition.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Model Training Performance Visualization\n",
      "======================================================================\n",
      "Base directory: C:\\Users\\Tian\\Desktop\\地磁论文代码运行测试\\results\n",
      "Models to process: GRUModel, LSTMModel, RNNModel, MLPModel, TransformerModel\n",
      "Time windows: 7day, 14day, 30day\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Checking model folders:\n",
      "  ✓ GRUModel             15 log files found\n",
      "  ✓ LSTMModel            15 log files found\n",
      "  ✓ RNNModel             15 log files found\n",
      "  ✓ MLPModel             15 log files found\n",
      "  ✓ TransformerModel     15 log files found\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Processing 7-day window:\n",
      "  Processing GRUModel... Found 5 log file(s) for GRUModel 7day\n",
      "✓\n",
      "  Processing LSTMModel... Found 5 log file(s) for LSTMModel 7day\n",
      "✓\n",
      "  Processing RNNModel... Found 5 log file(s) for RNNModel 7day\n",
      "✓\n",
      "  Processing MLPModel... Found 5 log file(s) for MLPModel 7day\n",
      "✓\n",
      "  Processing TransformerModel... Found 5 log file(s) for TransformerModel 7day\n",
      "✓\n",
      "  Generating plot for 5 models...\n",
      "Successfully saved: Model_Performance_7-day.png\n",
      "\n",
      "Processing 14-day window:\n",
      "  Processing GRUModel... Found 5 log file(s) for GRUModel 14day\n",
      "✓\n",
      "  Processing LSTMModel... Found 5 log file(s) for LSTMModel 14day\n",
      "✓\n",
      "  Processing RNNModel... Found 5 log file(s) for RNNModel 14day\n",
      "✓\n",
      "  Processing MLPModel... Found 5 log file(s) for MLPModel 14day\n",
      "✓\n",
      "  Processing TransformerModel... Found 5 log file(s) for TransformerModel 14day\n",
      "✓\n",
      "  Generating plot for 5 models...\n",
      "Successfully saved: Model_Performance_14-day.png\n",
      "\n",
      "Processing 30-day window:\n",
      "  Processing GRUModel... Found 5 log file(s) for GRUModel 30day\n",
      "✓\n",
      "  Processing LSTMModel... Found 5 log file(s) for LSTMModel 30day\n",
      "✓\n",
      "  Processing RNNModel... Found 5 log file(s) for RNNModel 30day\n",
      "✓\n",
      "  Processing MLPModel... Found 5 log file(s) for MLPModel 30day\n",
      "✓\n",
      "  Processing TransformerModel... Found 5 log file(s) for TransformerModel 30day\n",
      "✓\n",
      "  Generating plot for 5 models...\n",
      "Successfully saved: Model_Performance_30-day.png\n",
      "\n",
      "======================================================================\n",
      "SUCCESS: Generated 3 visualization(s)\n",
      "Output directory: C:\\Users\\Tian\\Desktop\\地磁论文代码运行测试\\results\\performance_visualization\\Performance_Analysis\n",
      "  - Model_Performance_7-day.png\n",
      "  - Model_Performance_14-day.png\n",
      "  - Model_Performance_30-day.png\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model Training Performance Visualization with Corrected Legend Positioning\n",
    "Supports Chinese paths using pathlib for cross-platform compatibility\n",
    "Legend boxes aligned with plot borders for professional appearance\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "# ================== Global Configuration ==================\n",
    "\n",
    "# Base directory using pathlib for better path handling\n",
    "BASE_DIR = Path(r\"your_project/results\")\n",
    "   # Change to your actual results directory\n",
    "   # For example, if your project is at: C:/Users/Tian/Desktop/地磁论文代码运行测试\n",
    "   # Then change to: output_dir: str = r\"C:\\Users\\Tian\\Desktop\\地磁论文代码运行测试\\results\"\n",
    "\n",
    "# Model names for ensemble learning\n",
    "MODEL_NAMES = [\"GRUModel\", \"LSTMModel\", \"RNNModel\", \"MLPModel\", \"TransformerModel\"]\n",
    "\n",
    "# Mapping of model names to their respective folder names\n",
    "MODEL_FOLDERS = {\n",
    "    \"GRUModel\": \"gru_models\",\n",
    "    \"LSTMModel\": \"lstm_models\",\n",
    "    \"RNNModel\": \"rnn_models\",\n",
    "    \"MLPModel\": \"mlp_models\",\n",
    "    \"TransformerModel\": \"transformer_models\"\n",
    "}\n",
    "\n",
    "# Time window configurations\n",
    "TIME_WINDOWS = [\"7day\", \"14day\", \"30day\"]\n",
    "WINDOW_DISPLAY = {\"7day\": \"7-day\", \"14day\": \"14-day\", \"30day\": \"30-day\"}\n",
    "\n",
    "# Color scheme for different models (scientific publication standard)\n",
    "COLORS = {\n",
    "    'GRU': '#E41A1C',         # Red\n",
    "    'LSTM': '#377EB8',        # Blue\n",
    "    'RNN': '#984EA3',         # Purple\n",
    "    'MLP': '#4DAF4A',         # Green\n",
    "    'Transformer': '#FF7F00'  # Orange\n",
    "}\n",
    "\n",
    "# Configure matplotlib for scientific journal style\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelsize': 32,\n",
    "    'axes.titlesize': 34,\n",
    "    'legend.fontsize': 21,\n",
    "    'xtick.labelsize': 26,\n",
    "    'ytick.labelsize': 26,\n",
    "    'axes.grid': False,\n",
    "    'figure.figsize': (18, 16),\n",
    "    'legend.frameon': True,\n",
    "    'legend.fancybox': False,\n",
    "    'legend.edgecolor': 'black',\n",
    "    'axes.linewidth': 1.5,\n",
    "    'lines.linewidth': 4.5,\n",
    "})\n",
    "\n",
    "\n",
    "def get_model_key(model_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the model key from the full model name.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Full model name (e.g., 'GRUModel')\n",
    "    \n",
    "    Returns:\n",
    "        Model key (e.g., 'GRU')\n",
    "    \"\"\"\n",
    "    return model_name.replace(\"Model\", \"\").strip()\n",
    "\n",
    "\n",
    "def read_ensemble_weights(model_name: str, window_name: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Read ensemble weights from the configuration file.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        window_name: Time window identifier\n",
    "    \n",
    "    Returns:\n",
    "        List of normalized ensemble weights, defaults to equal weights if not found\n",
    "    \"\"\"\n",
    "    folder_path = BASE_DIR / MODEL_FOLDERS[model_name]\n",
    "    config_filename = f\"{model_name}_{window_name}_ensemble_config.json\"\n",
    "    config_path = folder_path / config_filename\n",
    "    \n",
    "    # Default equal weights for 5 folds\n",
    "    default_weights = [0.2] * 5\n",
    "    \n",
    "    if not config_path.exists():\n",
    "        print(f\"Config not found: {config_path.name}, using equal weights\")\n",
    "        return default_weights\n",
    "    \n",
    "    try:\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        weights = config.get(\"ensemble_weights\", [])\n",
    "        if not weights:\n",
    "            return default_weights\n",
    "        \n",
    "        # Normalize weights to sum to 1\n",
    "        total = sum(weights)\n",
    "        if total <= 0:\n",
    "            return default_weights\n",
    "        \n",
    "        return [w / total for w in weights]\n",
    "    \n",
    "    except (json.JSONDecodeError, IOError) as e:\n",
    "        print(f\"Error reading config {config_path.name}: {e}\")\n",
    "        return default_weights\n",
    "\n",
    "\n",
    "def parse_training_log(log_file: Path) -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Parse training log file to extract epoch metrics.\n",
    "    \n",
    "    Args:\n",
    "        log_file: Path to the log file\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing lists of epochs, losses, and accuracies\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    if not log_file.exists():\n",
    "        print(f\"Log file not found: {log_file.name}\")\n",
    "        return data\n",
    "    \n",
    "    try:\n",
    "        with open(log_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Regular expressions for extracting metrics\n",
    "        patterns = {\n",
    "            'epoch': r'Epoch\\s+(\\d+)/',\n",
    "            'train_loss': r'Train Loss:\\s+([\\d\\.]+)',\n",
    "            'train_acc': r'Train Acc:\\s+([\\d\\.]+)',\n",
    "            'val_loss': r'Val Loss:\\s+([\\d\\.]+)',\n",
    "            'val_acc': r'Val Acc:\\s+([\\d\\.]+)'\n",
    "        }\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.startswith('Epoch'):\n",
    "                # Extract all metrics from the line\n",
    "                matches = {key: re.search(pattern, line) \n",
    "                          for key, pattern in patterns.items()}\n",
    "                \n",
    "                # Only add data if all metrics are found\n",
    "                if all(matches.values()):\n",
    "                    data['epoch'].append(int(matches['epoch'].group(1)))\n",
    "                    data['train_loss'].append(float(matches['train_loss'].group(1)))\n",
    "                    data['train_acc'].append(float(matches['train_acc'].group(1)))\n",
    "                    data['val_loss'].append(float(matches['val_loss'].group(1)))\n",
    "                    data['val_acc'].append(float(matches['val_acc'].group(1)))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing log file {log_file.name}: {e}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def find_fold_logs(model_name: str, window_name: str) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Find all fold log files for a specific model and time window.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        window_name: Time window identifier\n",
    "    \n",
    "    Returns:\n",
    "        Sorted list of log file paths\n",
    "    \"\"\"\n",
    "    folder_path = BASE_DIR / MODEL_FOLDERS[model_name]\n",
    "    \n",
    "    if not folder_path.exists():\n",
    "        print(f\"Model folder does not exist: {folder_path}\")\n",
    "        return []\n",
    "    \n",
    "    # Pattern for fold log files\n",
    "    pattern = f\"{model_name}_{window_name}_fold_*_logs.txt\"\n",
    "    \n",
    "    # Use pathlib's glob method for better path handling\n",
    "    log_files = sorted(folder_path.glob(pattern))\n",
    "    \n",
    "    if not log_files:\n",
    "        print(f\"No log files found matching pattern: {pattern}\")\n",
    "        # List available files for debugging\n",
    "        available_logs = list(folder_path.glob(\"*_logs.txt\"))\n",
    "        if available_logs:\n",
    "            print(f\"Available log files: {[f.name for f in available_logs[:3]]}\")\n",
    "    else:\n",
    "        print(f\"Found {len(log_files)} log file(s) for {model_name} {window_name}\")\n",
    "    \n",
    "    return log_files\n",
    "\n",
    "\n",
    "def calculate_weighted_statistics(\n",
    "    data_array: np.ndarray, \n",
    "    weights: List[float]\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate weighted mean and standard error for ensemble data.\n",
    "    \n",
    "    Args:\n",
    "        data_array: 2D array (folds x epochs)\n",
    "        weights: Weights for each fold\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (weighted means, standard errors)\n",
    "    \"\"\"\n",
    "    n_folds, n_epochs = data_array.shape\n",
    "    means = np.zeros(n_epochs)\n",
    "    errors = np.zeros(n_epochs)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        values = data_array[:, epoch]\n",
    "        # Weighted mean\n",
    "        weighted_mean = np.sum(np.array(weights) * values)\n",
    "        # Weighted variance\n",
    "        weighted_var = np.sum(np.array(weights) * (values - weighted_mean) ** 2)\n",
    "        # Standard error\n",
    "        std_error = np.sqrt(weighted_var) / np.sqrt(n_folds)\n",
    "        \n",
    "        means[epoch] = weighted_mean\n",
    "        errors[epoch] = std_error\n",
    "    \n",
    "    return means, errors\n",
    "\n",
    "\n",
    "def combine_fold_results(model_name: str, window_name: str) -> Optional[Dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Combine results from multiple folds using weighted averaging.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        window_name: Time window identifier\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing combined metrics or None if no data available\n",
    "    \"\"\"\n",
    "    weights = read_ensemble_weights(model_name, window_name)\n",
    "    log_files = find_fold_logs(model_name, window_name)\n",
    "    \n",
    "    if not log_files:\n",
    "        print(f\"No log files found for {model_name} {window_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Parse all fold data\n",
    "    folds_data = []\n",
    "    for log_file in log_files:\n",
    "        parsed = parse_training_log(log_file)\n",
    "        if parsed['epoch']:\n",
    "            folds_data.append(parsed)\n",
    "    \n",
    "    if not folds_data:\n",
    "        print(f\"No valid data found in log files for {model_name} {window_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Adjust weights to match actual number of folds\n",
    "    n_folds = len(folds_data)\n",
    "    used_weights = weights[:n_folds]\n",
    "    \n",
    "    # Normalize weights\n",
    "    weight_sum = sum(used_weights)\n",
    "    if weight_sum <= 0:\n",
    "        used_weights = [1.0 / n_folds] * n_folds\n",
    "    else:\n",
    "        used_weights = [w / weight_sum for w in used_weights]\n",
    "    \n",
    "    # Find minimum number of epochs across all folds\n",
    "    min_epochs = min(len(fd['epoch']) for fd in folds_data)\n",
    "    \n",
    "    # Create arrays for all metrics\n",
    "    metrics = {\n",
    "        'train_loss': np.zeros((n_folds, min_epochs)),\n",
    "        'val_loss': np.zeros((n_folds, min_epochs)),\n",
    "        'train_acc': np.zeros((n_folds, min_epochs)),\n",
    "        'val_acc': np.zeros((n_folds, min_epochs))\n",
    "    }\n",
    "    \n",
    "    # Fill arrays with fold data\n",
    "    for i, fold_data in enumerate(folds_data):\n",
    "        for metric in metrics:\n",
    "            metrics[metric][i, :] = fold_data[metric][:min_epochs]\n",
    "    \n",
    "    # Calculate weighted statistics for each metric\n",
    "    combined = {'epoch': np.arange(1, min_epochs + 1)}\n",
    "    \n",
    "    for metric_name, metric_data in metrics.items():\n",
    "        mean, error = calculate_weighted_statistics(metric_data, used_weights)\n",
    "        combined[f'{metric_name}_mean'] = mean\n",
    "        combined[f'{metric_name}_err'] = error\n",
    "    \n",
    "    return combined\n",
    "\n",
    "\n",
    "def create_performance_plots(window_name: str, results_dict: Dict[str, Dict]) -> None:\n",
    "    \"\"\"\n",
    "    Create 2x2 subplot of training and validation metrics with transparent legend background.\n",
    "    \n",
    "    Args:\n",
    "        window_name: Time window identifier\n",
    "        results_dict: Dictionary mapping model names to their combined results\n",
    "    \"\"\"\n",
    "    window_display = WINDOW_DISPLAY.get(window_name, window_name)\n",
    "    \n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(25, 18))\n",
    "    \n",
    "    # Configure subplot properties\n",
    "    subplot_config = [\n",
    "        {'ax': axes[0, 0], 'title': 'Training Accuracy', 'metric': 'train_acc', 'style': '-'},\n",
    "        {'ax': axes[0, 1], 'title': 'Validation Accuracy', 'metric': 'val_acc', 'style': '--'},\n",
    "        {'ax': axes[1, 0], 'title': 'Training Loss', 'metric': 'train_loss', 'style': '-'},\n",
    "        {'ax': axes[1, 1], 'title': 'Validation Loss', 'metric': 'val_loss', 'style': '--'}\n",
    "    ]\n",
    "    \n",
    "    # Collect data ranges for dynamic y-axis scaling\n",
    "    data_ranges = {\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'train_loss': [], 'val_loss': []\n",
    "    }\n",
    "    \n",
    "    for model_name, data in results_dict.items():\n",
    "        for metric in data_ranges:\n",
    "            data_ranges[metric].extend(data[f'{metric}_mean'])\n",
    "    \n",
    "    # Set up each subplot\n",
    "    for config in subplot_config:\n",
    "        ax = config['ax']\n",
    "        ax.set_xlabel('Epoch', fontweight='bold')\n",
    "        ax.set_ylabel(config['title'], fontweight='bold')\n",
    "        ax.tick_params(direction='in', width=1.5)\n",
    "        \n",
    "        # Set dynamic y-axis range with 10% padding\n",
    "        metric = config['metric']\n",
    "        if data_ranges[metric]:\n",
    "            data_min = min(data_ranges[metric])\n",
    "            data_max = max(data_ranges[metric])\n",
    "            data_range = data_max - data_min\n",
    "            ax.set_ylim(data_min - data_range * 0.1, data_max + data_range * 0.1)\n",
    "        \n",
    "        # Configure spines\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(True)\n",
    "            spine.set_linewidth(1.5)\n",
    "    \n",
    "    # Track final validation accuracy for legend sorting\n",
    "    model_performance = {}\n",
    "    \n",
    "    # Plot data for each model\n",
    "    for model_name, data in results_dict.items():\n",
    "        model_key = get_model_key(model_name)\n",
    "        color = COLORS.get(model_key, '#000000')\n",
    "        \n",
    "        epochs = data['epoch']\n",
    "        \n",
    "        # Extract final epoch values for legend labels\n",
    "        final_values = {\n",
    "            'train_acc': data['train_acc_mean'][-1],\n",
    "            'val_acc': data['val_acc_mean'][-1],\n",
    "            'train_loss': data['train_loss_mean'][-1],\n",
    "            'val_loss': data['val_loss_mean'][-1]\n",
    "        }\n",
    "        \n",
    "        model_performance[model_key] = final_values['val_acc']\n",
    "        \n",
    "        # Plot on each subplot\n",
    "        for config in subplot_config:\n",
    "            ax = config['ax']\n",
    "            metric = config['metric']\n",
    "            y_data = data[f'{metric}_mean']\n",
    "            \n",
    "            # Create label with final value\n",
    "            label = f\"{model_key} ({final_values[metric]:.3f})\"\n",
    "            \n",
    "            # Plot line\n",
    "            ax.plot(epochs, y_data, \n",
    "                   color=color, \n",
    "                   linestyle=config['style'],\n",
    "                   linewidth=4.5,\n",
    "                   label=label)\n",
    "    \n",
    "    # Sort legends by validation accuracy (descending) and position at plot borders\n",
    "    for i, config in enumerate(subplot_config):\n",
    "        ax = config['ax']\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        \n",
    "        if handles:\n",
    "            # Extract model names and sort by performance\n",
    "            model_names = [label.split()[0] for label in labels]\n",
    "            sorted_indices = sorted(range(len(model_names)),\n",
    "                                  key=lambda idx: model_performance.get(model_names[idx], 0),\n",
    "                                  reverse=True)\n",
    "            \n",
    "            # Position legend at plot borders based on subplot position\n",
    "            if i == 0:  # Training accuracy - upper right border\n",
    "                loc = 'upper right'\n",
    "                bbox = (1.0, 1.0)\n",
    "            elif i == 1:  # Validation accuracy - upper right border\n",
    "                loc = 'upper right'\n",
    "                bbox = (1.0, 1.0)\n",
    "            elif i == 2:  # Training loss - lower right border\n",
    "                loc = 'lower right'\n",
    "                bbox = (1.0, 0.0)\n",
    "            else:  # Validation loss - lower right border\n",
    "                loc = 'lower right'\n",
    "                # Special adjustment for 30-day window\n",
    "                if window_name == \"30day\":\n",
    "                    bbox = (1.0, 0.35)\n",
    "                else:\n",
    "                    bbox = (1.0, 0.0)\n",
    "            \n",
    "            # Create sorted legend aligned with plot border\n",
    "            legend = ax.legend([handles[idx] for idx in sorted_indices],\n",
    "                             [labels[idx] for idx in sorted_indices],\n",
    "                             loc=loc,\n",
    "                             bbox_to_anchor=bbox,\n",
    "                             frameon=True,\n",
    "                             borderaxespad=0.0)  # No padding from border\n",
    "            \n",
    "            # Style legend with transparent background and visible border\n",
    "            legend.get_frame().set_edgecolor('black')\n",
    "            legend.get_frame().set_linewidth(1.0)\n",
    "            legend.get_frame().set_facecolor('none')  # Transparent background\n",
    "            legend.get_frame().set_alpha(1.0)  # Full opacity for border\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.subplots_adjust(left=0.06, right=0.94, bottom=0.06, top=0.94, wspace=0.25, hspace=0.25)\n",
    "    \n",
    "    # Save figure\n",
    "    save_dir = BASE_DIR / \"performance_visualization\" / \"Performance_Analysis\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    output_file = save_dir / f\"Model_Performance_{window_display}.png\"\n",
    "    \n",
    "    try:\n",
    "        plt.savefig(output_file, dpi=600, bbox_inches='tight')\n",
    "        print(f\"Successfully saved: {output_file.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving figure: {e}\")\n",
    "        # Try alternative save location\n",
    "        alt_dir = Path(\"C:/temp/results\")\n",
    "        alt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        alt_file = alt_dir / f\"Model_Performance_{window_display}.png\"\n",
    "        plt.savefig(alt_file, dpi=600, bbox_inches='tight')\n",
    "        print(f\"Saved to alternative location: {alt_file}\")\n",
    "    \n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function for generating performance visualization plots.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Model Training Performance Visualization\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Verify base directory exists\n",
    "    if not BASE_DIR.exists():\n",
    "        print(f\"ERROR: Base directory does not exist: {BASE_DIR}\")\n",
    "        print(\"Please check the path configuration.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Base directory: {BASE_DIR}\")\n",
    "    print(f\"Models to process: {', '.join(MODEL_NAMES)}\")\n",
    "    print(f\"Time windows: {', '.join(TIME_WINDOWS)}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Check available model folders\n",
    "    print(\"\\nChecking model folders:\")\n",
    "    for model_name, folder_name in MODEL_FOLDERS.items():\n",
    "        folder_path = BASE_DIR / folder_name\n",
    "        if folder_path.exists():\n",
    "            log_count = len(list(folder_path.glob(\"*_logs.txt\")))\n",
    "            print(f\"  ✓ {model_name:20} {log_count} log files found\")\n",
    "        else:\n",
    "            print(f\"  ✗ {model_name:20} folder not found\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Process each time window\n",
    "    generated_files = []\n",
    "    \n",
    "    for window in TIME_WINDOWS:\n",
    "        print(f\"\\nProcessing {WINDOW_DISPLAY[window]} window:\")\n",
    "        \n",
    "        # Collect results for all models\n",
    "        results = {}\n",
    "        \n",
    "        for model in MODEL_NAMES:\n",
    "            print(f\"  Processing {model}...\", end=\" \")\n",
    "            combined_data = combine_fold_results(model, window)\n",
    "            \n",
    "            if combined_data is not None:\n",
    "                results[model] = combined_data\n",
    "                print(\"✓\")\n",
    "            else:\n",
    "                print(\"✗ (no data)\")\n",
    "        \n",
    "        # Generate plot if we have results\n",
    "        if results:\n",
    "            print(f\"  Generating plot for {len(results)} models...\")\n",
    "            create_performance_plots(window, results)\n",
    "            generated_files.append(f\"Model_Performance_{WINDOW_DISPLAY[window]}.png\")\n",
    "        else:\n",
    "            print(f\"  No valid data for {WINDOW_DISPLAY[window]} window, skipping plot.\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    if generated_files:\n",
    "        save_dir = BASE_DIR / \"performance_visualization\" / \"Performance_Analysis\"\n",
    "        print(f\"SUCCESS: Generated {len(generated_files)} visualization(s)\")\n",
    "        print(f\"Output directory: {save_dir}\")\n",
    "        for filename in generated_files:\n",
    "            print(f\"  - {filename}\")\n",
    "    else:\n",
    "        print(\"WARNING: No visualizations were generated.\")\n",
    "        print(\"Please check that log files exist and are properly formatted.\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2:Model effect comparison under different time windows\n",
    "\n",
    "This module provides comprehensive comparative analysis and visualization of model performance across different temporal windows (7-day, 14-day, 30-day) for seismic-geomagnetic signal recognition. It generates radar charts and scatter plots to evaluate model effectiveness, stability, and trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Model Performance Comparison Visualization\n",
      "======================================================================\n",
      "Base directory: C:\\Users\\Tian\\Desktop\\地磁论文代码运行测试\\results\n",
      "Output directory: C:\\Users\\Tian\\Desktop\\地磁论文代码运行测试\\results\\performance_visualization\\Model_Comparison_Analysis\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Loading model configurations...\n",
      "Loaded: GRUModel_7day_ensemble_config.json\n",
      "Loaded: GRUModel_14day_ensemble_config.json\n",
      "Loaded: GRUModel_30day_ensemble_config.json\n",
      "Loaded: LSTMModel_7day_ensemble_config.json\n",
      "Loaded: LSTMModel_14day_ensemble_config.json\n",
      "Loaded: LSTMModel_30day_ensemble_config.json\n",
      "Loaded: MLPModel_7day_ensemble_config.json\n",
      "Loaded: MLPModel_14day_ensemble_config.json\n",
      "Loaded: MLPModel_30day_ensemble_config.json\n",
      "Loaded: RNNModel_7day_ensemble_config.json\n",
      "Loaded: RNNModel_14day_ensemble_config.json\n",
      "Loaded: RNNModel_30day_ensemble_config.json\n",
      "Loaded: TransformerModel_7day_ensemble_config.json\n",
      "Loaded: TransformerModel_14day_ensemble_config.json\n",
      "Loaded: TransformerModel_30day_ensemble_config.json\n",
      "\n",
      "Extracting performance metrics...\n",
      "\n",
      "Generating visualizations...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Radar Charts:\n",
      "Generated: radar_chart_1_7day.png\n",
      "Generated: radar_chart_2_14day.png\n",
      "Generated: radar_chart_3_30day.png\n",
      "\n",
      "Scatter Plots:\n",
      "Generated: scatter_4_7day_stability.png\n",
      "Generated: scatter_5_7day_correlation.png\n",
      "Generated: scatter_6_7day_tradeoff.png\n",
      "Generated: scatter_7_14day_stability.png\n",
      "Generated: scatter_8_14day_correlation.png\n",
      "Generated: scatter_9_14day_tradeoff.png\n",
      "Generated: scatter_10_30day_stability.png\n",
      "Generated: scatter_11_30day_correlation.png\n",
      "Generated: scatter_12_30day_tradeoff.png\n",
      "\n",
      "Legend Figures:\n",
      "Generated: legend_radar.png\n",
      "Generated: legend_scatter.png\n",
      "\n",
      "======================================================================\n",
      "SUCCESS: Generated all 14 visualization figures\n",
      "Output location: C:\\Users\\Tian\\Desktop\\地磁论文代码运行测试\\results\\performance_visualization\\Model_Comparison_Analysis\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model Performance Comparison with Radar Charts and Scatter Plots\n",
    "Supports Chinese paths using pathlib for cross-platform compatibility\n",
    "Generates comprehensive performance visualizations for multiple models\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from scipy import stats\n",
    "\n",
    "# Configure matplotlib for scientific publication style\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.labelsize': 26,\n",
    "    'axes.titlesize': 26,\n",
    "    'legend.fontsize': 16,\n",
    "    'xtick.labelsize': 22,\n",
    "    'ytick.labelsize': 22,\n",
    "    'axes.grid': False,\n",
    "    'legend.frameon': False,\n",
    "})\n",
    "\n",
    "# Color scheme for different models\n",
    "COLORS = {\n",
    "    'GRU': '#E41A1C',         # Red\n",
    "    'LSTM': '#377EB8',        # Blue\n",
    "    'MLP': '#4DAF4A',         # Green\n",
    "    'RNN': '#984EA3',         # Purple\n",
    "    'Transformer': '#FF7F00'  # Orange\n",
    "}\n",
    "\n",
    "# Marker styles for scatter plots\n",
    "MARKERS = {\n",
    "    'GRU': 'o',\n",
    "    'LSTM': 's',\n",
    "    'MLP': '^',\n",
    "    'RNN': 'v',\n",
    "    'Transformer': 'X'\n",
    "}\n",
    "\n",
    "# Metric name mappings for internal consistency\n",
    "METRIC_MAPPING = {\n",
    "    'F1 Score': 'f1',\n",
    "    'Precision': 'precision',\n",
    "    'Recall': 'recall',\n",
    "    'Specificity': 'specificity',\n",
    "    'Norm MCC': 'norm_mcc'\n",
    "}\n",
    "\n",
    "# Model folder structure\n",
    "MODEL_FOLDERS = {\n",
    "    \"GRU\": \"gru_models\",\n",
    "    \"LSTM\": \"lstm_models\",\n",
    "    \"MLP\": \"mlp_models\",\n",
    "    \"RNN\": \"rnn_models\",\n",
    "    \"Transformer\": \"transformer_models\"\n",
    "}\n",
    "\n",
    "\n",
    "def format_tick_label(value: float) -> str:\n",
    "    \"\"\"\n",
    "    Format tick label to avoid -0.00 display issue.\n",
    "    \n",
    "    Args:\n",
    "        value: The numeric value to format\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string representation\n",
    "    \"\"\"\n",
    "    # Handle near-zero values\n",
    "    if abs(value) < 0.005:\n",
    "        return \"0.00\"\n",
    "    return f\"{value:.2f}\"\n",
    "\n",
    "\n",
    "def load_model_configurations(BASE_DIR: Path) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Load all model configuration files from the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        BASE_DIR: Base directory containing model folders\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing model configurations organized by model and window\n",
    "    \"\"\"\n",
    "    model_names = [\"GRU\", \"LSTM\", \"MLP\", \"RNN\", \"Transformer\"]\n",
    "    window_periods = [\"7day\", \"14day\", \"30day\"]\n",
    "    all_model_data = {}\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        folder_name = MODEL_FOLDERS.get(model_name)\n",
    "        if not folder_name:\n",
    "            continue\n",
    "        \n",
    "        model_path = BASE_DIR / folder_name\n",
    "        if not model_path.exists():\n",
    "            print(f\"Model folder not found: {model_path}\")\n",
    "            continue\n",
    "        \n",
    "        all_model_data[model_name] = {}\n",
    "        \n",
    "        for window_period in window_periods:\n",
    "            config_file = model_path / f\"{model_name}Model_{window_period}_ensemble_config.json\"\n",
    "            \n",
    "            if config_file.exists():\n",
    "                try:\n",
    "                    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "                        config = json.load(f)\n",
    "                    all_model_data[model_name][window_period] = config\n",
    "                    print(f\"Loaded: {config_file.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {config_file.name}: {e}\")\n",
    "    \n",
    "    return all_model_data\n",
    "\n",
    "\n",
    "def extract_performance_metrics(model_data: Dict) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Extract performance metrics from model configuration data.\n",
    "    \n",
    "    Args:\n",
    "        model_data: Raw model configuration data\n",
    "    \n",
    "    Returns:\n",
    "        Organized performance metrics by model and window\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    for model_name, windows in model_data.items():\n",
    "        metrics[model_name] = {}\n",
    "        \n",
    "        for window_period, config in windows.items():\n",
    "            metrics[model_name][window_period] = {}\n",
    "            \n",
    "            if 'average_metrics' in config:\n",
    "                metrics_data = config['average_metrics']\n",
    "                \n",
    "                # Extract mean values\n",
    "                metrics[model_name][window_period]['f1'] = metrics_data.get('avg_f1_scores', 0)\n",
    "                metrics[model_name][window_period]['precision'] = metrics_data.get('avg_precisions', 0)\n",
    "                metrics[model_name][window_period]['recall'] = metrics_data.get('avg_recalls', 0)\n",
    "                metrics[model_name][window_period]['specificity'] = metrics_data.get('avg_specificities', 0)\n",
    "                metrics[model_name][window_period]['mcc'] = metrics_data.get('avg_mccs', 0)\n",
    "                metrics[model_name][window_period]['norm_mcc'] = metrics_data.get('avg_norm_mccs', 0)\n",
    "                \n",
    "                # Extract standard deviations\n",
    "                metrics[model_name][window_period]['f1_std'] = metrics_data.get('std_f1_scores', 0.01)\n",
    "                metrics[model_name][window_period]['precision_std'] = metrics_data.get('std_precisions', 0.01)\n",
    "                metrics[model_name][window_period]['recall_std'] = metrics_data.get('std_recalls', 0.01)\n",
    "                metrics[model_name][window_period]['specificity_std'] = metrics_data.get('std_specificities', 0.01)\n",
    "                metrics[model_name][window_period]['mcc_std'] = metrics_data.get('std_mccs', 0.01)\n",
    "                metrics[model_name][window_period]['norm_mcc_std'] = metrics_data.get('std_norm_mccs', 0.01)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def create_radar_chart(window_data: Dict, metric_names: List[str], output_file: Path) -> None:\n",
    "    \"\"\"\n",
    "    Create a radar chart for comparing model performance across multiple metrics.\n",
    "    \n",
    "    Args:\n",
    "        window_data: Performance data for all models in a specific window\n",
    "        metric_names: List of metric names to display\n",
    "        output_file: Path to save the output figure\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10, 10), dpi=300)\n",
    "    ax = fig.add_subplot(111, projection='polar')\n",
    "    \n",
    "    # Prepare angular positions\n",
    "    models = list(window_data.keys())\n",
    "    num_metrics = len(metric_names)\n",
    "    theta = np.linspace(0, 2 * np.pi, num_metrics, endpoint=False).tolist()\n",
    "    theta += theta[:1]  # Close the polygon\n",
    "    \n",
    "    # Configure radar chart appearance\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_facecolor(\"#ffffff\")\n",
    "    \n",
    "    # Configure grid\n",
    "    ax.grid(True, linestyle='-', linewidth=0.5, color='gray', alpha=0.3)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_rticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.tick_params(axis='y', labelsize=22)\n",
    "    \n",
    "    # Hide default angular labels\n",
    "    ax.set_xticks(theta[:-1])\n",
    "    ax.set_xticklabels([])\n",
    "    \n",
    "    # Add metric names with position adjustments\n",
    "    for i, (angle, metric) in enumerate(zip(theta[:-1], metric_names)):\n",
    "        # Fine-tune positions for overlapping labels\n",
    "        if metric == 'Norm MCC':\n",
    "            adjusted_angle = angle - 0.1\n",
    "        elif metric == 'Precision':\n",
    "            adjusted_angle = angle + 0.1\n",
    "        else:\n",
    "            adjusted_angle = angle\n",
    "        \n",
    "        ax.text(adjusted_angle, 1.15, metric, \n",
    "                ha='center', va='center', fontsize=26, fontweight='bold')\n",
    "    \n",
    "    # Plot each model's performance\n",
    "    for model in models:\n",
    "        values = []\n",
    "        for metric in metric_names:\n",
    "            metric_key = METRIC_MAPPING.get(metric, metric.lower().replace(' ', '_'))\n",
    "            if metric_key in window_data[model]:\n",
    "                values.append(window_data[model][metric_key])\n",
    "            else:\n",
    "                values.append(0.0)\n",
    "        \n",
    "        values_closed = values + values[:1]\n",
    "        color = COLORS.get(model, '#000000')\n",
    "        \n",
    "        ax.plot(theta, values_closed,\n",
    "                color=color, linewidth=2.5,\n",
    "                marker='o', markersize=10,\n",
    "                label=model)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close(fig)\n",
    "    print(f\"Generated: {output_file.name}\")\n",
    "\n",
    "\n",
    "def create_scatter_plot(window_data: Dict, x_metric: str, y_metric: str, \n",
    "                       models: List[str], output_file: Path) -> None:\n",
    "    \"\"\"\n",
    "    Create a scatter plot comparing two metrics across models.\n",
    "    \n",
    "    Args:\n",
    "        window_data: Performance data for all models\n",
    "        x_metric: Metric for x-axis\n",
    "        y_metric: Metric for y-axis\n",
    "        models: List of model names\n",
    "        output_file: Path to save the output figure\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), dpi=300)\n",
    "    \n",
    "    # Collect data points\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    model_names = []\n",
    "    \n",
    "    for model in models:\n",
    "        if x_metric in window_data[model] and y_metric in window_data[model]:\n",
    "            x_values.append(window_data[model][x_metric])\n",
    "            y_values.append(window_data[model][y_metric])\n",
    "            model_names.append(model)\n",
    "    \n",
    "    # Plot scatter points\n",
    "    for i, model in enumerate(model_names):\n",
    "        ax.scatter(x_values[i], y_values[i],\n",
    "                  s=400, c='none',\n",
    "                  marker=MARKERS.get(model, 'o'),\n",
    "                  edgecolors=COLORS.get(model, 'black'),\n",
    "                  linewidth=2.5, label=model)\n",
    "    \n",
    "    # Format axis labels\n",
    "    x_label = x_metric.replace('_', ' ').title().replace('F 1', 'F1')\n",
    "    y_label = y_metric.replace('_', ' ').title().replace('F 1', 'F1')\n",
    "    \n",
    "    ax.set_xlabel(x_label, fontsize=26, fontweight='bold')\n",
    "    ax.set_ylabel(y_label, fontsize=26, fontweight='bold')\n",
    "    \n",
    "    # Set axis ticks with proper formatting\n",
    "    if len(x_values) > 0:\n",
    "        x_min, x_max = min(x_values), max(x_values)\n",
    "        x_margin = (x_max - x_min) * 0.05\n",
    "        x_ticks = np.linspace(x_min - x_margin, x_max + x_margin, 5)\n",
    "        ax.set_xticks(x_ticks)\n",
    "        ax.set_xticklabels([format_tick_label(tick) for tick in x_ticks])\n",
    "        \n",
    "        y_min, y_max = min(y_values), max(y_values)\n",
    "        y_margin = (y_max - y_min) * 0.05\n",
    "        y_ticks = np.linspace(y_min - y_margin, y_max + y_margin, 5)\n",
    "        ax.set_yticks(y_ticks)\n",
    "        ax.set_yticklabels([format_tick_label(tick) for tick in y_ticks])\n",
    "        \n",
    "        ax.set_xlim(x_min - x_margin, x_max + x_margin)\n",
    "        ax.set_ylim(y_min - y_margin, y_max + y_margin)\n",
    "    \n",
    "    # Configure spines\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(1.5)\n",
    "        spine.set_edgecolor('black')\n",
    "    \n",
    "    # Add diagonal reference line for correlation analysis\n",
    "    if x_metric != f'{x_metric}_std' and y_metric != f'{y_metric}_std':\n",
    "        lims = [\n",
    "            np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "            np.max([ax.get_xlim(), ax.get_ylim()]),\n",
    "        ]\n",
    "        ax.plot(lims, lims, 'k--', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close(fig)\n",
    "    print(f\"Generated: {output_file.name}\")\n",
    "\n",
    "\n",
    "def create_stability_plot(window_data: Dict, metric: str, models: List[str], \n",
    "                         output_file: Path) -> None:\n",
    "    \"\"\"\n",
    "    Create a stability scatter plot showing mean vs standard deviation.\n",
    "    \n",
    "    Args:\n",
    "        window_data: Performance data for all models\n",
    "        metric: Base metric name\n",
    "        models: List of model names\n",
    "        output_file: Path to save the output figure\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), dpi=300)\n",
    "    \n",
    "    # Collect mean and std values\n",
    "    x_values = []  # mean values\n",
    "    y_values = []  # std values\n",
    "    model_names = []\n",
    "    \n",
    "    for model in models:\n",
    "        if metric in window_data[model] and f\"{metric}_std\" in window_data[model]:\n",
    "            x_values.append(window_data[model][metric])\n",
    "            y_values.append(window_data[model][f\"{metric}_std\"])\n",
    "            model_names.append(model)\n",
    "    \n",
    "    # Plot scatter points\n",
    "    for i, model in enumerate(model_names):\n",
    "        ax.scatter(x_values[i], y_values[i],\n",
    "                  s=400, c='none',\n",
    "                  marker=MARKERS.get(model, 'o'),\n",
    "                  edgecolors=COLORS.get(model, 'black'),\n",
    "                  linewidth=2.5, label=model)\n",
    "    \n",
    "    # Format labels\n",
    "    metric_name = metric.replace('_', ' ').title().replace('F 1', 'F1')\n",
    "    ax.set_xlabel(f'Mean {metric_name}', fontsize=26, fontweight='bold')\n",
    "    ax.set_ylabel(f'{metric_name} S.D.', fontsize=26, fontweight='bold')\n",
    "    \n",
    "    # Set axis ticks with proper formatting\n",
    "    if len(x_values) > 0:\n",
    "        x_min, x_max = min(x_values), max(x_values)\n",
    "        x_margin = (x_max - x_min) * 0.05\n",
    "        x_ticks = np.linspace(x_min - x_margin, x_max + x_margin, 5)\n",
    "        ax.set_xticks(x_ticks)\n",
    "        ax.set_xticklabels([format_tick_label(tick) for tick in x_ticks])\n",
    "        \n",
    "        y_min, y_max = min(y_values), max(y_values)\n",
    "        y_margin = (y_max - y_min) * 0.05\n",
    "        y_ticks = np.linspace(y_min - y_margin, y_max + y_margin, 5)\n",
    "        ax.set_yticks(y_ticks)\n",
    "        ax.set_yticklabels([format_tick_label(tick) for tick in y_ticks])\n",
    "        \n",
    "        ax.set_xlim(x_min - x_margin, x_max + x_margin)\n",
    "        ax.set_ylim(y_min - y_margin, y_max + y_margin)\n",
    "    \n",
    "    # Configure spines\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(1.5)\n",
    "        spine.set_edgecolor('black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close(fig)\n",
    "    print(f\"Generated: {output_file.name}\")\n",
    "\n",
    "\n",
    "def create_legend_figures(output_dir: Path) -> None:\n",
    "    \"\"\"\n",
    "    Create separate legend figures for radar and scatter plots.\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory to save the legend figures\n",
    "    \"\"\"\n",
    "    # Legend for radar charts (line style)\n",
    "    fig, ax = plt.subplots(figsize=(12, 1.5), dpi=300)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    lines = []\n",
    "    for model, color in COLORS.items():\n",
    "        line = plt.Line2D([0], [0], color=color, linewidth=3,\n",
    "                         marker='o', markersize=12, label=model)\n",
    "        lines.append(line)\n",
    "    \n",
    "    ax.legend(handles=lines, loc='center', ncol=5, fontsize=20,\n",
    "             frameon=False, columnspacing=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    radar_legend_path = output_dir / 'legend_radar.png'\n",
    "    plt.savefig(radar_legend_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close(fig)\n",
    "    print(f\"Generated: {radar_legend_path.name}\")\n",
    "    \n",
    "    # Legend for scatter plots (marker style)\n",
    "    fig, ax = plt.subplots(figsize=(12, 1.5), dpi=300)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    scatter_handles = []\n",
    "    for model, color in COLORS.items():\n",
    "        handle = plt.Line2D([0], [0], marker=MARKERS[model], color='w',\n",
    "                           markerfacecolor='none', markersize=18,\n",
    "                           markeredgecolor=color, markeredgewidth=2.5,\n",
    "                           linestyle='None', label=model)\n",
    "        scatter_handles.append(handle)\n",
    "    \n",
    "    ax.legend(handles=scatter_handles, loc='center', ncol=5, fontsize=20,\n",
    "             frameon=False, columnspacing=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    scatter_legend_path = output_dir / 'legend_scatter.png'\n",
    "    plt.savefig(scatter_legend_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close(fig)\n",
    "    print(f\"Generated: {scatter_legend_path.name}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function to generate all visualization figures.\n",
    "    \"\"\"\n",
    "    # Configure paths using pathlib\n",
    "    BASE_DIR = Path(r\"your_project/results\")\n",
    "    # Change to your actual results directory\n",
    "    # For example, if your project is at: C:/Users/Tian/Desktop/地磁论文代码运行测试\n",
    "    # Then change to: BASE_DIR = Path(r\"C:\\Users\\Tian\\Desktop\\地磁论文代码运行测试\\results\")\n",
    "    output_dir = BASE_DIR / \"performance_visualization\" / \"Model_Comparison_Analysis\"\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Model Performance Comparison Visualization\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Base directory: {BASE_DIR}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Load and process data\n",
    "    print(\"\\nLoading model configurations...\")\n",
    "    model_data = load_model_configurations(BASE_DIR)\n",
    "    \n",
    "    if not model_data:\n",
    "        print(\"ERROR: No model data found. Please check the base directory.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nExtracting performance metrics...\")\n",
    "    metrics = extract_performance_metrics(model_data)\n",
    "    \n",
    "    # Configuration\n",
    "    radar_metrics = ['F1 Score', 'Precision', 'Recall', 'Specificity', 'Norm MCC']\n",
    "    window_periods = [\"7day\", \"14day\", \"30day\"]\n",
    "    window_display = {\"7day\": \"7-day\", \"14day\": \"14-day\", \"30day\": \"30-day\"}\n",
    "    \n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Generate radar charts (3 total)\n",
    "    print(\"\\nRadar Charts:\")\n",
    "    for idx, window in enumerate(window_periods, 1):\n",
    "        window_metrics = {}\n",
    "        for model, model_windows in metrics.items():\n",
    "            if window in model_windows:\n",
    "                window_metrics[model] = model_windows[window]\n",
    "        \n",
    "        if window_metrics:\n",
    "            output_file = output_dir / f'radar_chart_{idx}_{window}.png'\n",
    "            create_radar_chart(window_metrics, radar_metrics, output_file)\n",
    "    \n",
    "    # Scatter plot configurations\n",
    "    scatter_configs = [\n",
    "        ('f1', 'f1_std', 'stability'),        # F1 stability analysis\n",
    "        ('norm_mcc', 'f1', 'correlation'),    # Norm MCC vs F1 correlation\n",
    "        ('precision', 'recall', 'tradeoff')   # Precision-Recall tradeoff\n",
    "    ]\n",
    "    \n",
    "    # Generate scatter plots (9 total: 3 types × 3 windows)\n",
    "    print(\"\\nScatter Plots:\")\n",
    "    plot_number = 4  # Start from 4 (radar charts are 1-3)\n",
    "    \n",
    "    for window in window_periods:\n",
    "        window_metrics = {}\n",
    "        available_models = []\n",
    "        \n",
    "        for model, model_windows in metrics.items():\n",
    "            if window in model_windows:\n",
    "                window_metrics[model] = model_windows[window]\n",
    "                available_models.append(model)\n",
    "        \n",
    "        if window_metrics:\n",
    "            for x_metric, y_metric, plot_type in scatter_configs:\n",
    "                output_file = output_dir / f'scatter_{plot_number}_{window}_{plot_type}.png'\n",
    "                \n",
    "                if plot_type == 'stability':\n",
    "                    create_stability_plot(window_metrics, x_metric, available_models, output_file)\n",
    "                else:\n",
    "                    create_scatter_plot(window_metrics, x_metric, y_metric, \n",
    "                                      available_models, output_file)\n",
    "                \n",
    "                plot_number += 1\n",
    "    \n",
    "    # Generate legend figures\n",
    "    print(\"\\nLegend Figures:\")\n",
    "    create_legend_figures(output_dir)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"SUCCESS: Generated all 14 visualization figures\")\n",
    "    print(f\"Output location: {output_dir}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: ROC Curve Analysis and Visualization\n",
    "\n",
    "This module generates comparative ROC curves for multiple deep learning models (GRU, LSTM, MLP, RNN, Transformer) across different temporal windows (7-day, 14-day, 30-day) for seismic-geomagnetic signal recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Multi-Model ROC Curve Analysis\n",
      "======================================================================\n",
      "Base directory: C:\\Users\\Tian\\Desktop\\地磁论文代码运行测试\\results\n",
      "Models to process: GRU, LSTM, MLP, RNN, Transformer\n",
      "Time windows: 7day, 14day, 30day\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Processing GRU model:\n",
      "  7-day window... ✓ (AUC: 0.977)\n",
      "  14-day window... ✓ (AUC: 0.951)\n",
      "  30-day window... ✓ (AUC: 0.954)\n",
      "\n",
      "Processing LSTM model:\n",
      "  7-day window... ✓ (AUC: 0.983)\n",
      "  14-day window... ✓ (AUC: 0.958)\n",
      "  30-day window... ✓ (AUC: 0.958)\n",
      "\n",
      "Processing MLP model:\n",
      "  7-day window... ✓ (AUC: 0.628)\n",
      "  14-day window... ✓ (AUC: 0.711)\n",
      "  30-day window... ✓ (AUC: 0.769)\n",
      "\n",
      "Processing RNN model:\n",
      "  7-day window... ✓ (AUC: 0.964)\n",
      "  14-day window... ✓ (AUC: 0.844)\n",
      "  30-day window... ✓ (AUC: 0.806)\n",
      "\n",
      "Processing Transformer model:\n",
      "  7-day window... ✓ (AUC: 0.489)\n",
      "  14-day window... ✓ (AUC: 0.530)\n",
      "  30-day window... ✓ (AUC: 0.581)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Successfully processed data for 5 model(s)\n",
      "Generating ROC curve visualization...\n",
      "\n",
      "SUCCESS: Figure saved to C:\\Users\\Tian\\Desktop\\地磁论文代码运行测试\\results\\performance_visualization\\ROC_analysis\\multi_model_roc_curves.png\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multi-Model ROC Curve Analysis with Professional Visualization\n",
    "Supports Chinese paths using pathlib for cross-platform compatibility\n",
    "Legend boxes with transparent background and visible borders\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from scipy.interpolate import interp1d\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# ================== Global Configuration ==================\n",
    "\n",
    "# Base directory using pathlib for better path handling\n",
    "BASE_DIR = Path(r\"your_project/results\")\n",
    "   # Change to your actual results directory\n",
    "   # For example, if your project is at: C:/Users/Tian/Desktop/地磁论文代码运行测试\n",
    "   # Then change to: output_dir: str = r\"C:\\Users\\Tian\\Desktop\\地磁论文代码运行测试\\results\"\n",
    "\n",
    "# Model configurations\n",
    "MODELS = [\"GRU\", \"LSTM\", \"MLP\", \"RNN\", \"Transformer\"]\n",
    "\n",
    "# Time window configurations  \n",
    "TIME_WINDOWS = [\"7day\", \"14day\", \"30day\"]\n",
    "WINDOW_DISPLAY = {\"7day\": \"7-day\", \"14day\": \"14-day\", \"30day\": \"30-day\"}\n",
    "\n",
    "# Number of cross-validation folds\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "# Color scheme for different models (scientific publication standard)\n",
    "COLORS = {\n",
    "    'GRU': '#E41A1C',         # Red\n",
    "    'LSTM': '#377EB8',        # Blue\n",
    "    'MLP': '#4DAF4A',         # Green\n",
    "    'RNN': '#984EA3',         # Purple\n",
    "    'Transformer': '#FF7F00'  # Orange\n",
    "}\n",
    "\n",
    "# Configure matplotlib for scientific journal style\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelsize': 24,\n",
    "    'axes.titlesize': 24,\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'legend.fontsize': 13.5,\n",
    "    'xtick.labelsize': 20,\n",
    "    'ytick.labelsize': 20,\n",
    "    'axes.grid': False,\n",
    "    'figure.figsize': (18, 6),\n",
    "    'legend.frameon': True,\n",
    "    'legend.fancybox': False,\n",
    "    'legend.edgecolor': 'black',\n",
    "    'axes.linewidth': 1.0,\n",
    "})\n",
    "\n",
    "# ================== Data Processing Functions ==================\n",
    "\n",
    "def load_fold_data(model: str, window: str, fold: int, base_dir: Path) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Load test labels and probabilities for a specific fold.\n",
    "    \n",
    "    Args:\n",
    "        model: Model name\n",
    "        window: Time window identifier\n",
    "        fold: Fold number (1-based)\n",
    "        base_dir: Base directory path\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (labels, probabilities) or (None, None) if files not found\n",
    "    \"\"\"\n",
    "    model_dir = base_dir / f\"{model.lower()}_models\"\n",
    "    \n",
    "    if not model_dir.exists():\n",
    "        print(f\"Directory not found: {model_dir}\")\n",
    "        return None, None\n",
    "    \n",
    "    labels_file = model_dir / f\"{model}Model_{window}_fold_{fold}_test_labels.npy\"\n",
    "    probs_file = model_dir / f\"{model}Model_{window}_fold_{fold}_test_probs.npy\"\n",
    "    \n",
    "    if not labels_file.exists() or not probs_file.exists():\n",
    "        print(f\"Missing data: {model} - {window} - Fold {fold}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        labels = np.load(labels_file)\n",
    "        probs = np.load(probs_file)\n",
    "        \n",
    "        # Extract positive class probabilities\n",
    "        if probs.ndim == 2 and probs.shape[1] == 2:\n",
    "            pos_probs = probs[:, 1]\n",
    "        else:\n",
    "            pos_probs = probs\n",
    "            \n",
    "        return labels, pos_probs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {model} - {window} - Fold {fold}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def calculate_fold_roc(labels: np.ndarray, probabilities: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Calculate ROC curve for a single fold.\n",
    "    \n",
    "    Args:\n",
    "        labels: True labels\n",
    "        probabilities: Predicted probabilities\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (FPR, TPR, AUC score)\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(labels, probabilities)\n",
    "    \n",
    "    # Ensure FPR is monotonically increasing\n",
    "    unique_fpr, unique_indices = np.unique(fpr, return_index=True)\n",
    "    unique_tpr = tpr[unique_indices]\n",
    "    \n",
    "    auc_score = roc_auc_score(labels, probabilities)\n",
    "    \n",
    "    return unique_fpr, unique_tpr, auc_score\n",
    "\n",
    "\n",
    "def interpolate_roc_curves(fprs: List[np.ndarray], tprs: List[np.ndarray]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Interpolate multiple ROC curves to common FPR points.\n",
    "    \n",
    "    Args:\n",
    "        fprs: List of FPR arrays\n",
    "        tprs: List of TPR arrays\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (mean FPR, mean TPR, std TPR)\n",
    "    \"\"\"\n",
    "    # Common FPR points for interpolation\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    interp_tprs = []\n",
    "    \n",
    "    for fpr, tpr in zip(fprs, tprs):\n",
    "        if len(fpr) > 1:\n",
    "            interp_func = interp1d(fpr, tpr, kind='linear', \n",
    "                                 bounds_error=False, fill_value=(0, 1))\n",
    "            interp_tprs.append(interp_func(mean_fpr))\n",
    "    \n",
    "    if not interp_tprs:\n",
    "        return mean_fpr, np.zeros_like(mean_fpr), np.zeros_like(mean_fpr)\n",
    "    \n",
    "    mean_tpr = np.mean(interp_tprs, axis=0)\n",
    "    std_tpr = np.std(interp_tprs, axis=0)\n",
    "    \n",
    "    return mean_fpr, mean_tpr, std_tpr\n",
    "\n",
    "\n",
    "def process_model_roc(model: str, window: str, base_dir: Path) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Process ROC curves for all folds of a model.\n",
    "    \n",
    "    Args:\n",
    "        model: Model name\n",
    "        window: Time window identifier\n",
    "        base_dir: Base directory path\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with ROC statistics or None if insufficient data\n",
    "    \"\"\"\n",
    "    fold_fprs = []\n",
    "    fold_tprs = []\n",
    "    fold_aucs = []\n",
    "    \n",
    "    # Process each fold\n",
    "    for fold in range(1, NUM_FOLDS + 1):\n",
    "        labels, probs = load_fold_data(model, window, fold, base_dir)\n",
    "        \n",
    "        if labels is None or probs is None:\n",
    "            continue\n",
    "        \n",
    "        fpr, tpr, auc = calculate_fold_roc(labels, probs)\n",
    "        \n",
    "        if len(fpr) > 1:\n",
    "            fold_fprs.append(fpr)\n",
    "            fold_tprs.append(tpr)\n",
    "            fold_aucs.append(auc)\n",
    "    \n",
    "    if not fold_fprs:\n",
    "        print(f\"Insufficient data for {model} in {window}\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate mean ROC curve\n",
    "    mean_fpr, mean_tpr, std_tpr = interpolate_roc_curves(fold_fprs, fold_tprs)\n",
    "    \n",
    "    return {\n",
    "        \"mean_fpr\": mean_fpr,\n",
    "        \"mean_tpr\": mean_tpr,\n",
    "        \"std_tpr\": std_tpr,\n",
    "        \"mean_auc\": np.mean(fold_aucs),\n",
    "        \"std_auc\": np.std(fold_aucs)\n",
    "    }\n",
    "\n",
    "\n",
    "# ================== Visualization Functions ==================\n",
    "\n",
    "def create_roc_subplot(ax: plt.Axes, window: str, model_data: Dict[str, Dict]) -> None:\n",
    "    \"\"\"\n",
    "    Create ROC curve plot for a specific time window.\n",
    "    \n",
    "    Args:\n",
    "        ax: Matplotlib axes object\n",
    "        window: Time window identifier\n",
    "        model_data: Dictionary containing ROC data for all models\n",
    "    \"\"\"\n",
    "    # Configure subplot borders\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(1.0)\n",
    "    \n",
    "    # Track AUC values for legend sorting\n",
    "    model_aucs = {}\n",
    "    \n",
    "    # Plot ROC curve for each model\n",
    "    for model in MODELS:\n",
    "        if model in model_data and window in model_data[model]:\n",
    "            roc_info = model_data[model][window]\n",
    "            \n",
    "            # Store AUC for sorting\n",
    "            model_aucs[model] = roc_info[\"mean_auc\"]\n",
    "            \n",
    "            # Plot ROC curve\n",
    "            ax.plot(\n",
    "                roc_info[\"mean_fpr\"], \n",
    "                roc_info[\"mean_tpr\"],\n",
    "                lw=2, \n",
    "                color=COLORS[model],\n",
    "                label=f'{model} ({roc_info[\"mean_auc\"]:.2f})'\n",
    "            )\n",
    "    \n",
    "    # Add diagonal reference line\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "    \n",
    "    # Configure axis properties\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate', fontweight='bold')\n",
    "    ax.set_ylabel('True Positive Rate', fontweight='bold')\n",
    "    ax.tick_params(direction='in')\n",
    "    \n",
    "    # Get current handles and labels for sorting\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    \n",
    "    if handles:\n",
    "        # Sort by AUC values (descending), keeping Random at the end\n",
    "        model_indices = []\n",
    "        random_idx = -1\n",
    "        \n",
    "        for idx, label in enumerate(labels):\n",
    "            if label == 'Random':\n",
    "                random_idx = idx\n",
    "            else:\n",
    "                model_name = label.split()[0]\n",
    "                model_indices.append((idx, model_aucs.get(model_name, 0)))\n",
    "        \n",
    "        # Sort model indices by AUC\n",
    "        model_indices.sort(key=lambda x: x[1], reverse=True)\n",
    "        sorted_indices = [idx for idx, _ in model_indices]\n",
    "        \n",
    "        # Add Random at the end if present\n",
    "        if random_idx >= 0:\n",
    "            sorted_indices.append(random_idx)\n",
    "        \n",
    "        # Create sorted legend with transparent background\n",
    "        legend = ax.legend(\n",
    "            [handles[idx] for idx in sorted_indices],\n",
    "            [labels[idx] for idx in sorted_indices],\n",
    "            loc='lower right',\n",
    "            bbox_to_anchor=(1.0, 0.0),\n",
    "            frameon=True,\n",
    "            borderaxespad=0.0\n",
    "        )\n",
    "        \n",
    "        # Style legend with transparent background\n",
    "        frame = legend.get_frame()\n",
    "        frame.set_edgecolor('black')\n",
    "        frame.set_linewidth(1.0)\n",
    "        frame.set_facecolor('none')  # Transparent background\n",
    "\n",
    "\n",
    "def create_multi_model_roc_figure(model_data: Dict[str, Dict]) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Create figure with ROC curves for all time windows.\n",
    "    \n",
    "    Args:\n",
    "        model_data: Dictionary containing ROC data for all models\n",
    "    \n",
    "    Returns:\n",
    "        Matplotlib figure object\n",
    "    \"\"\"\n",
    "    # Create figure with three subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n",
    "    \n",
    "    # Configuration for subplot labels\n",
    "    label_positions = [\n",
    "        {'x': 0.01, 'y': 0.95},  # Position for 'a'\n",
    "        {'x': 0.35, 'y': 0.95},  # Position for 'b'  \n",
    "        {'x': 0.69, 'y': 0.95}   # Position for 'c'\n",
    "    ]\n",
    "    label_fontsize = 20\n",
    "    \n",
    "    # Generate subplot for each time window\n",
    "    for idx, window in enumerate(TIME_WINDOWS):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Add subplot label\n",
    "        fig.text(\n",
    "            label_positions[idx]['x'],\n",
    "            label_positions[idx]['y'],\n",
    "            f'{chr(97+idx)}',\n",
    "            fontsize=label_fontsize,\n",
    "            fontweight='bold',\n",
    "            va='bottom',\n",
    "            ha='left'\n",
    "        )\n",
    "        \n",
    "        # Create ROC plot for this window\n",
    "        create_roc_subplot(ax, window, model_data)\n",
    "    \n",
    "    # Adjust subplot spacing\n",
    "    plt.subplots_adjust(wspace=0.30, left=0.05, right=0.98, bottom=0.15, top=0.95)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# ================== Main Execution Function ==================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function for generating ROC curve analysis.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Multi-Model ROC Curve Analysis\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Verify base directory exists\n",
    "    if not BASE_DIR.exists():\n",
    "        print(f\"ERROR: Base directory does not exist: {BASE_DIR}\")\n",
    "        print(\"Please check the path configuration.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Base directory: {BASE_DIR}\")\n",
    "    print(f\"Models to process: {', '.join(MODELS)}\")\n",
    "    print(f\"Time windows: {', '.join(TIME_WINDOWS)}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Initialize storage for all models' ROC data\n",
    "    model_roc_data = {}\n",
    "    \n",
    "    # Process each model\n",
    "    for model in MODELS:\n",
    "        print(f\"\\nProcessing {model} model:\")\n",
    "        model_roc_data[model] = {}\n",
    "        \n",
    "        for window in TIME_WINDOWS:\n",
    "            print(f\"  {WINDOW_DISPLAY[window]} window...\", end=\" \")\n",
    "            \n",
    "            roc_stats = process_model_roc(model, window, BASE_DIR)\n",
    "            \n",
    "            if roc_stats is not None:\n",
    "                model_roc_data[model][window] = roc_stats\n",
    "                print(f\"✓ (AUC: {roc_stats['mean_auc']:.3f})\")\n",
    "            else:\n",
    "                print(\"✗ (insufficient data)\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    \n",
    "    # Check if we have any valid data\n",
    "    valid_models = sum(1 for model_data in model_roc_data.values() if model_data)\n",
    "    \n",
    "    if valid_models == 0:\n",
    "        print(\"ERROR: No valid ROC data found for any model.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Successfully processed data for {valid_models} model(s)\")\n",
    "    print(\"Generating ROC curve visualization...\")\n",
    "    \n",
    "    # Create and save figure\n",
    "    fig = create_multi_model_roc_figure(model_roc_data)\n",
    "    \n",
    "    # Create output directory and save figure\n",
    "    save_dir = BASE_DIR / \"performance_visualization\" / \"ROC_analysis\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    output_file = save_dir / \"multi_model_roc_curves.png\"\n",
    "    \n",
    "    try:\n",
    "        plt.savefig(output_file, dpi=600, bbox_inches='tight')\n",
    "        print(f\"\\nSUCCESS: Figure saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR saving figure: {e}\")\n",
    "        # Try alternative save location\n",
    "        alt_dir = Path(\"C:/temp/results\")\n",
    "        alt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        alt_file = alt_dir / \"multi_model_roc_curves.png\"\n",
    "        plt.savefig(alt_file, dpi=600, bbox_inches='tight')\n",
    "        print(f\"Saved to alternative location: {alt_file}\")\n",
    "    \n",
    "    # Close figure to free memory\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Confusion Matrix Analysis with Cross-Validation Averaging\n",
    "\n",
    "This module generates individual confusion matrices for multiple deep learning models (LSTM, GRU, RNN, MLP, Transformer) across different temporal windows (7-day, 14-day, 30-day), using averaged results from 5-fold cross-validation for robust performance evaluation of seismic-geomagnetic signal classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Individual Confusion Matrix Analysis (Averaged from Cross-Validation)\n",
      "================================================================================\n",
      "Base directory: C:\\Users\\Tian\\Desktop\\地磁论文代码运行测试\\results\n",
      "Models: LSTM, GRU, RNN, MLP, Transformer\n",
      "Time windows: 7-day, 14-day, 30-day\n",
      "Output: Individual confusion matrix plots\n",
      "--------------------------------------------------------------------------------\n",
      "Calculating global maximum for color scale...\n",
      "  7-day: 5/5 folds averaged\n",
      "  14-day: 5/5 folds averaged\n",
      "  30-day: 5/5 folds averaged\n",
      "  7-day: 5/5 folds averaged\n",
      "  14-day: 5/5 folds averaged\n",
      "  30-day: 5/5 folds averaged\n",
      "  7-day: 5/5 folds averaged\n",
      "  14-day: 5/5 folds averaged\n",
      "  30-day: 5/5 folds averaged\n",
      "  7-day: 5/5 folds averaged\n",
      "  14-day: 5/5 folds averaged\n",
      "  30-day: 5/5 folds averaged\n",
      "  7-day: 5/5 folds averaged\n",
      "  14-day: 5/5 folds averaged\n",
      "  30-day: 5/5 folds averaged\n",
      "Global maximum value: 374.0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Processing LSTM model:\n",
      "  7-day: 5/5 folds averaged\n",
      "    Saved: cm_LSTM_7-day.png\n",
      "  14-day: 5/5 folds averaged\n",
      "    Saved: cm_LSTM_14-day.png\n",
      "  30-day: 5/5 folds averaged\n",
      "    Saved: cm_LSTM_30-day.png\n",
      "\n",
      "Processing GRU model:\n",
      "  7-day: 5/5 folds averaged\n",
      "    Saved: cm_GRU_7-day.png\n",
      "  14-day: 5/5 folds averaged\n",
      "    Saved: cm_GRU_14-day.png\n",
      "  30-day: 5/5 folds averaged\n",
      "    Saved: cm_GRU_30-day.png\n",
      "\n",
      "Processing RNN model:\n",
      "  7-day: 5/5 folds averaged\n",
      "    Saved: cm_RNN_7-day.png\n",
      "  14-day: 5/5 folds averaged\n",
      "    Saved: cm_RNN_14-day.png\n",
      "  30-day: 5/5 folds averaged\n",
      "    Saved: cm_RNN_30-day.png\n",
      "\n",
      "Processing MLP model:\n",
      "  7-day: 5/5 folds averaged\n",
      "    Saved: cm_MLP_7-day.png\n",
      "  14-day: 5/5 folds averaged\n",
      "    Saved: cm_MLP_14-day.png\n",
      "  30-day: 5/5 folds averaged\n",
      "    Saved: cm_MLP_30-day.png\n",
      "\n",
      "Processing Transformer model:\n",
      "  7-day: 5/5 folds averaged\n",
      "    Saved: cm_Transformer_7-day.png\n",
      "  14-day: 5/5 folds averaged\n",
      "    Saved: cm_Transformer_14-day.png\n",
      "  30-day: 5/5 folds averaged\n",
      "    Saved: cm_Transformer_30-day.png\n",
      "\n",
      "Saved colorbar: horizontal_colorbar.png\n",
      "\n",
      "================================================================================\n",
      "SUCCESS: Generated 15/15 confusion matrix plots\n",
      "Output directory: C:\\Users\\Tian\\Desktop\\地磁论文代码运行测试\\results\\performance_visualization\\Individual_Confusion_Matrices\n",
      "\n",
      "Generated files:\n",
      "  - cm_LSTM_7-day.png\n",
      "  - cm_LSTM_14-day.png\n",
      "  - cm_LSTM_30-day.png\n",
      "  - cm_GRU_7-day.png\n",
      "  - cm_GRU_14-day.png\n",
      "  - cm_GRU_30-day.png\n",
      "  - cm_RNN_7-day.png\n",
      "  - cm_RNN_14-day.png\n",
      "  - cm_RNN_30-day.png\n",
      "  - cm_MLP_7-day.png\n",
      "  - cm_MLP_14-day.png\n",
      "  - cm_MLP_30-day.png\n",
      "  - cm_Transformer_7-day.png\n",
      "  - cm_Transformer_14-day.png\n",
      "  - cm_Transformer_30-day.png\n",
      "  - horizontal_colorbar.png\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multi-Model Confusion Matrix Analysis with Individual Plots\n",
    "Supports Chinese paths using pathlib for cross-platform compatibility\n",
    "Generates averaged confusion matrices from cross-validation folds\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# ================== Global Configuration ==================\n",
    "\n",
    "# Base directory using pathlib for better path handling\n",
    "BASE_DIR = Path(r\"your_project/results\")\n",
    "   # Change to your actual results directory\n",
    "   # For example, if your project is at: C:/Users/Tian/Desktop/地磁论文代码运行测试\n",
    "   # Then change to: output_dir: str = r\"C:\\Users\\Tian\\Desktop\\地磁论文代码运行测试\\results\"\n",
    "\n",
    "# Model configurations\n",
    "MODELS = [\"LSTM\", \"GRU\", \"RNN\", \"MLP\", \"Transformer\"]\n",
    "\n",
    "# Time window configurations\n",
    "TIME_WINDOWS = [\"7day\", \"14day\", \"30day\"]\n",
    "WINDOW_DISPLAY = {\"7day\": \"7-day\", \"14day\": \"14-day\", \"30day\": \"30-day\"}\n",
    "\n",
    "# Number of cross-validation folds\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "# Configure matplotlib for scientific journal style\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelsize': 50,\n",
    "    'axes.titlesize': 52,\n",
    "    'legend.fontsize': 50,\n",
    "    'xtick.labelsize': 50,\n",
    "    'ytick.labelsize': 50,\n",
    "    'axes.grid': False,\n",
    "    'figure.dpi': 100,\n",
    "})\n",
    "\n",
    "# Figure configuration\n",
    "FIG_SIZE = (10, 10)\n",
    "MARGINS = [0.16, 0.132, 0.80, 0.80]  # [left, bottom, width, height]\n",
    "\n",
    "# Heatmap configuration\n",
    "HEATMAP_CONFIG = {\n",
    "    'cmap': 'coolwarm',\n",
    "    'linewidths': 0.5,\n",
    "    'linecolor': 'gray',\n",
    "    'square': True,\n",
    "    'cbar': False,\n",
    "    'annot': False\n",
    "}\n",
    "\n",
    "# Text annotation configuration\n",
    "TEXT_CONFIG = {\n",
    "    'ha': 'center',\n",
    "    'va': 'center',\n",
    "    'color': 'black',\n",
    "    'fontsize': 90,\n",
    "    'fontweight': 'bold',\n",
    "    'fontfamily': 'Arial'\n",
    "}\n",
    "\n",
    "# ================== Data Loading Functions ==================\n",
    "\n",
    "def load_fold_confusion_matrix(model: str, window: str, fold: int, base_dir: Path) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load test data and calculate confusion matrix for a specific fold.\n",
    "    \n",
    "    Args:\n",
    "        model: Model name (e.g., 'LSTM')\n",
    "        window: Time window identifier (e.g., '7day')\n",
    "        fold: Fold number (1-based)\n",
    "        base_dir: Base directory path\n",
    "    \n",
    "    Returns:\n",
    "        Confusion matrix or None if files not found\n",
    "    \"\"\"\n",
    "    model_dir = base_dir / f\"{model.lower()}_models\"\n",
    "    \n",
    "    # File paths for test data (validation set)\n",
    "    probs_file = model_dir / f\"{model}Model_{window}_fold_{fold}_test_probs.npy\"\n",
    "    labels_file = model_dir / f\"{model}Model_{window}_fold_{fold}_test_labels.npy\"\n",
    "    \n",
    "    if not probs_file.exists() or not labels_file.exists():\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load probabilities and labels\n",
    "        probs = np.load(probs_file)\n",
    "        labels = np.load(labels_file)\n",
    "        \n",
    "        # Convert probabilities to predictions\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        cm = confusion_matrix(labels, preds)\n",
    "        \n",
    "        return cm\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading fold {fold} for {model} - {window}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_averaged_confusion_matrix(model: str, window: str, base_dir: Path) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate averaged confusion matrix across all folds.\n",
    "    \n",
    "    Args:\n",
    "        model: Model name\n",
    "        window: Time window identifier\n",
    "        base_dir: Base directory path\n",
    "    \n",
    "    Returns:\n",
    "        Averaged confusion matrix or None if no data\n",
    "    \"\"\"\n",
    "    fold_cms = []\n",
    "    \n",
    "    # Collect confusion matrices from all folds\n",
    "    for fold in range(1, NUM_FOLDS + 1):\n",
    "        cm = load_fold_confusion_matrix(model, window, fold, base_dir)\n",
    "        if cm is not None:\n",
    "            fold_cms.append(cm)\n",
    "        else:\n",
    "            print(f\"  Missing data: {model} - {WINDOW_DISPLAY[window]} - Fold {fold}\")\n",
    "    \n",
    "    if not fold_cms:\n",
    "        print(f\"  No valid data for {model} - {WINDOW_DISPLAY[window]}\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate average confusion matrix\n",
    "    avg_cm = np.mean(fold_cms, axis=0)\n",
    "    \n",
    "    print(f\"  {WINDOW_DISPLAY[window]}: {len(fold_cms)}/{NUM_FOLDS} folds averaged\")\n",
    "    \n",
    "    return avg_cm\n",
    "\n",
    "\n",
    "# ================== Global Maximum Calculation ==================\n",
    "\n",
    "def find_global_maximum(base_dir: Path) -> float:\n",
    "    \"\"\"\n",
    "    Find the global maximum value across all averaged confusion matrices.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Base directory path\n",
    "    \n",
    "    Returns:\n",
    "        Global maximum value\n",
    "    \"\"\"\n",
    "    max_value = 0\n",
    "    \n",
    "    print(\"Calculating global maximum for color scale...\")\n",
    "    \n",
    "    for model in MODELS:\n",
    "        for window in TIME_WINDOWS:\n",
    "            avg_cm = calculate_averaged_confusion_matrix(model, window, base_dir)\n",
    "            if avg_cm is not None:\n",
    "                max_value = max(max_value, avg_cm.max())\n",
    "    \n",
    "    print(f\"Global maximum value: {max_value:.1f}\")\n",
    "    return max_value\n",
    "\n",
    "\n",
    "# ================== Visualization Functions ==================\n",
    "\n",
    "def create_individual_confusion_matrix(\n",
    "    model: str, \n",
    "    window: str, \n",
    "    avg_cm: np.ndarray,\n",
    "    output_dir: Path\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create and save an individual confusion matrix plot.\n",
    "    \n",
    "    Args:\n",
    "        model: Model name\n",
    "        window: Time window identifier\n",
    "        avg_cm: Averaged confusion matrix\n",
    "        output_dir: Output directory path\n",
    "    \"\"\"\n",
    "    # Create figure with fixed size\n",
    "    fig = plt.figure(figsize=FIG_SIZE, dpi=100)\n",
    "    plt.subplots_adjust(top=0.88, bottom=0.07, left=0.14, right=0.92)\n",
    "    \n",
    "    # Add axes with specified margins\n",
    "    ax = fig.add_axes(MARGINS)\n",
    "    \n",
    "    # Round values for display\n",
    "    avg_cm_int = np.round(avg_cm).astype(int)\n",
    "    \n",
    "    # Determine axis labels (only show for specific positions)\n",
    "    xticklabels = [\"Class 0\", \"Class 1\"] if model == \"Transformer\" else [\"\", \"\"]\n",
    "    yticklabels = [\"Class 0\", \"Class 1\"] if window == \"7day\" else [\"\", \"\"]\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        avg_cm,\n",
    "        ax=ax,\n",
    "        xticklabels=xticklabels,\n",
    "        yticklabels=yticklabels,\n",
    "        **HEATMAP_CONFIG\n",
    "    )\n",
    "    \n",
    "    # Manually add text annotations\n",
    "    for i in range(avg_cm_int.shape[0]):\n",
    "        for j in range(avg_cm_int.shape[1]):\n",
    "            ax.text(j + 0.5, i + 0.5, str(avg_cm_int[i, j]), **TEXT_CONFIG)\n",
    "    \n",
    "    # Force update\n",
    "    plt.draw()\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    # Set axis labels conditionally\n",
    "    if window == \"7day\":\n",
    "        ax.set_ylabel('Actual Label', fontweight='bold', fontsize=50, labelpad=5)\n",
    "    else:\n",
    "        ax.set_ylabel('')\n",
    "    \n",
    "    if model == \"Transformer\":\n",
    "        ax.set_xlabel('Predicted Label', fontweight='bold', fontsize=50, labelpad=5)\n",
    "    else:\n",
    "        ax.set_xlabel('')\n",
    "    \n",
    "    # Set tick parameters\n",
    "    ax.tick_params(axis='both', labelsize=50)\n",
    "    \n",
    "    # Add title\n",
    "    plt.title(f\"{model} - {WINDOW_DISPLAY[window]}\", fontweight='bold', fontsize=52, pad=10)\n",
    "    \n",
    "    # Ensure borders are visible\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(1.0)\n",
    "    \n",
    "    # Save figure with fixed dimensions\n",
    "    output_file = output_dir / f\"cm_{model}_{WINDOW_DISPLAY[window]}.png\"\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches=None)\n",
    "    print(f\"    Saved: {output_file.name}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def create_horizontal_colorbar(max_value: float, output_dir: Path) -> None:\n",
    "    \"\"\"\n",
    "    Create and save a horizontal colorbar.\n",
    "    \n",
    "    Args:\n",
    "        max_value: Maximum value for color scale\n",
    "        output_dir: Output directory path\n",
    "    \"\"\"\n",
    "    # Create figure for colorbar\n",
    "    fig_colorbar = plt.figure(figsize=(12, 2), dpi=100)\n",
    "    \n",
    "    # Define ticks\n",
    "    max_val_rounded = max(350, int(np.ceil(max_value)))\n",
    "    ticks = np.array([0, 50, 100, 150, 200, 250, 300, 350])\n",
    "    \n",
    "    # Add additional ticks if needed\n",
    "    if max_val_rounded > 350:\n",
    "        additional_ticks = np.arange(400, max_val_rounded + 50, 50)\n",
    "        ticks = np.append(ticks, additional_ticks)\n",
    "    \n",
    "    # Create color mapping\n",
    "    norm = mpl.colors.Normalize(vmin=0, vmax=max_val_rounded)\n",
    "    cmap = plt.cm.coolwarm\n",
    "    \n",
    "    # Create horizontal colorbar\n",
    "    ax_cbar = fig_colorbar.add_axes([0.1, 0.4, 0.8, 0.3])\n",
    "    cb = mpl.colorbar.ColorbarBase(\n",
    "        ax_cbar, \n",
    "        cmap=cmap, \n",
    "        norm=norm,\n",
    "        orientation='horizontal', \n",
    "        ticks=ticks\n",
    "    )\n",
    "    \n",
    "    # Set tick labels\n",
    "    cb.ax.set_xticklabels([str(int(tick)) for tick in ticks], fontsize=50, fontweight='bold')\n",
    "    cb.ax.tick_params(length=6, width=2)\n",
    "    cb.set_label('Number of Samples', fontsize=52, fontweight='bold', labelpad=15)\n",
    "    \n",
    "    # Save colorbar\n",
    "    colorbar_file = output_dir / \"horizontal_colorbar.png\"\n",
    "    plt.savefig(colorbar_file, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nSaved colorbar: {colorbar_file.name}\")\n",
    "    plt.close(fig_colorbar)\n",
    "\n",
    "\n",
    "# ================== Main Execution Function ==================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function for generating individual confusion matrix plots.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Individual Confusion Matrix Analysis (Averaged from Cross-Validation)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Verify base directory exists\n",
    "    if not BASE_DIR.exists():\n",
    "        print(f\"ERROR: Base directory does not exist: {BASE_DIR}\")\n",
    "        print(\"Please check the path configuration.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Base directory: {BASE_DIR}\")\n",
    "    print(f\"Models: {', '.join(MODELS)}\")\n",
    "    print(f\"Time windows: {', '.join([WINDOW_DISPLAY[w] for w in TIME_WINDOWS])}\")\n",
    "    print(f\"Output: Individual confusion matrix plots\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = BASE_DIR / \"performance_visualization\" / \"Individual_Confusion_Matrices\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Find global maximum for consistent color scale\n",
    "    max_value = find_global_maximum(BASE_DIR)\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Process each model and time window\n",
    "    successful_plots = 0\n",
    "    total_plots = len(MODELS) * len(TIME_WINDOWS)\n",
    "    \n",
    "    for model in MODELS:\n",
    "        print(f\"\\nProcessing {model} model:\")\n",
    "        \n",
    "        # Check if model directory exists\n",
    "        model_dir = BASE_DIR / f\"{model.lower()}_models\"\n",
    "        if not model_dir.exists():\n",
    "            print(f\"  WARNING: Model directory not found: {model_dir}\")\n",
    "            continue\n",
    "        \n",
    "        for window in TIME_WINDOWS:\n",
    "            # Calculate averaged confusion matrix\n",
    "            avg_cm = calculate_averaged_confusion_matrix(model, window, BASE_DIR)\n",
    "            \n",
    "            if avg_cm is not None:\n",
    "                # Create individual plot\n",
    "                create_individual_confusion_matrix(model, window, avg_cm, output_dir)\n",
    "                successful_plots += 1\n",
    "            else:\n",
    "                print(f\"    Skipped: No data available\")\n",
    "    \n",
    "    # Create horizontal colorbar\n",
    "    if successful_plots > 0:\n",
    "        create_horizontal_colorbar(max_value, output_dir)\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    if successful_plots > 0:\n",
    "        print(f\"SUCCESS: Generated {successful_plots}/{total_plots} confusion matrix plots\")\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "        print(\"\\nGenerated files:\")\n",
    "        \n",
    "        # List generated files\n",
    "        for model in MODELS:\n",
    "            for window in TIME_WINDOWS:\n",
    "                filename = f\"cm_{model}_{WINDOW_DISPLAY[window]}.png\"\n",
    "                if (output_dir / filename).exists():\n",
    "                    print(f\"  - {filename}\")\n",
    "        \n",
    "        print(\"  - horizontal_colorbar.png\")\n",
    "    else:\n",
    "        print(\"WARNING: No visualizations were generated.\")\n",
    "        print(\"Please check that test result files exist.\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
